{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b423924",
   "metadata": {},
   "source": [
    "# ðŸ“Š Bank Earnings Sentiment & Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86dede",
   "metadata": {},
   "source": [
    "This notebook analyzes the sentiment and latent topics from earnings call transcripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b4e0c",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c105fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"transcript_sentences.csv\")\n",
    "df['quarter_year'] = df['quarter'] + \" \" + df['year'].astype(str)\n",
    "df = df.dropna(subset=['sentence'])  # Remove empty rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61b6a80",
   "metadata": {},
   "source": [
    "## ðŸ˜Š Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['sentiment_score'] = df['sentence'].apply(lambda x: sia.polarity_scores(str(x))['compound'])\n",
    "df['sentiment_label'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0.2 else 'negative' if x < -0.2 else 'neutral')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45986c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sentiment over time\n",
    "sentiment_summary = df.groupby(['quarter_year', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "\n",
    "sentiment_summary.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='coolwarm', title='Sentiment by Quarter')\n",
    "plt.ylabel(\"Sentence Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54641f51",
   "metadata": {},
   "source": [
    "## ðŸ§  Topic Modeling (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(df['sentence'].astype(str))\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "topics = lda.fit_transform(X)\n",
    "df['topic'] = topics.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show top words per topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
    "    print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c6c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Topic distribution\n",
    "topic_dist = df.groupby(['quarter_year', 'topic']).size().unstack(fill_value=0)\n",
    "topic_dist.plot(kind='bar', stacked=True, colormap='tab10', figsize=(12,6), title=\"Topics by Quarter\")\n",
    "plt.ylabel(\"Sentence Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340394c3",
   "metadata": {},
   "source": [
    "## ðŸ›ï¸ Regulatory Topic Mapping by LDA Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Regulatory mentions by topic\n",
    "reg_cols = [col for col in df.columns if col.startswith(\"Mentions \")]\n",
    "df['topic'] = df['topic'].astype(int)\n",
    "\n",
    "topic_reg_summary = (\n",
    "    df.groupby(\"topic\")[reg_cols]\n",
    "    .apply(lambda group: group.eq(\"Yes\").sum())\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Normalize by row for proportion\n",
    "topic_reg_percent = topic_reg_summary.div(topic_reg_summary.sum(axis=1), axis=0)\n",
    "\n",
    "# Heatmap of regulatory-topic associations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(topic_reg_percent, annot=True, cmap=\"Blues\", fmt=\".2f\")\n",
    "plt.title(\"ðŸ“Œ Regulatory Theme Association by Topic\")\n",
    "plt.ylabel(\"LDA Topic\")\n",
    "plt.xlabel(\"Regulatory Phrase\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
