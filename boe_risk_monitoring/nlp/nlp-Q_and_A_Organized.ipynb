{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "93c44503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import TextGeneration\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc54b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **SET GLOBAL VARIABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "28cb0c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial working directory:  C:\\\n",
      "New working directory:  C:\\\n"
     ]
    }
   ],
   "source": [
    "# Make sure the working directory is set correctly\n",
    "print(\"Initial working directory: \", os.getcwd())\n",
    "os.chdir(\"../..\")\n",
    "print(\"New working directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bd60728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FPATH = os.getcwd()\n",
    "DATA_FOLDER = \"data\"\n",
    "AGGREGATED_DATA_FOLDER_NAME = \"aggregated\"\n",
    "transcripts_FNAME = \"transcripts.parquet\"\n",
    "transcripts_FPATH = f\"{ROOT_FPATH}/{DATA_FOLDER}/{AGGREGATED_DATA_FOLDER_NAME}/{transcripts_FNAME}\"\n",
    "GLOSSARY_FNAME = \"glossary_dictionary_citigroup.csv\"\n",
    "GLOSSARY_FPATH = f\"{ROOT_FPATH}/{DATA_FOLDER}/{GLOSSARY_FNAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a91f96f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\/data/aggregated/transcripts.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read in the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_transcripts \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscripts_FPATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df_transcripts\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parquet.py:503\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;124;03mDataFrame\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    501\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    504\u001b[0m     path,\n\u001b[0;32m    505\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    506\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    507\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    509\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parquet.py:244\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    242\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilesystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    252\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    253\u001b[0m     )\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parquet.py:102\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m     92\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\/data/aggregated/transcripts.parquet'"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "df_transcripts = pd.read_parquet(transcripts_FPATH)\n",
    "df_transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87f72b",
   "metadata": {},
   "source": [
    "#### ðŸ˜Š Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a12f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check we don't have any missing data\n",
    "df_transcripts['text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b84ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use finbert-tone for sentiment analysis which is the finetuned version of BERT for financial sentiment analysis\n",
    "model_name = \"yiyanghkust/finbert-tone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07799b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check max token length for the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "print(model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max token length in the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "max_tokens = df_transcripts[\"text\"].apply(\n",
    "    lambda x: len(tokenizer(x, truncation=False)[\"input_ids\"])\n",
    ").max()\n",
    "print(\"Estimated max tokens in dataset:\", max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the reviews into two parts for parallel processing (2x GPUs)\n",
    "text_list = df_transcripts['text'].tolist()\n",
    "# Split list into 2 roughly equal parts\n",
    "midpoint = len(text_list) // 2\n",
    "text_list_split1 = text_list[:midpoint]\n",
    "text_list_split2 = text_list[midpoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee80631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sentiment_analysis_on_gpu(text_subset, model_name, device_id):\n",
    "\t\"\"\"\n",
    "\tRun sentiment analysis on a subset of text using a specified model and device.\n",
    "\t\"\"\"\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\tmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\tclf = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=device_id)\n",
    "\n",
    "\tresults = []\n",
    "\tbatch_size = 32\n",
    "\n",
    "\tfor i in range(0, len(text_subset), batch_size):\n",
    "\t\tbatch = text_subset[i:i+batch_size]\n",
    "\t\tbatch_results = clf(batch, truncation=True, max_length=512)\n",
    "\t\tresults.extend(batch_results)\n",
    "\t\tprint(f\"Device {device_id}: Processed {i + batch_size} reviews out of {len(text_subset)}\")\n",
    "\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3815de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sentiment analysis on both subsets in parallel using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    future_0 = executor.submit(run_sentiment_analysis_on_gpu, text_list_split1, model_name, 0)\n",
    "    future_1 = executor.submit(run_sentiment_analysis_on_gpu, text_list_split2, model_name, 1)\n",
    "\n",
    "    results_0 = future_0.result()\n",
    "    results_1 = future_1.result()\n",
    "    results = results_0 + results_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532ddac",
   "metadata": {},
   "source": [
    "## General banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocess results and add to DataFrame\n",
    "results_clean = [d['label'] for d in results]\n",
    "df_transcripts['sentiment'] = results_clean\n",
    "# Sentiment over time\n",
    "sentiment_summary = df_transcripts.groupby(['reporting_period', 'sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "# Step 1: Create a mapping from reporting_period to date_of_call\n",
    "# We'll take the first call date per reporting_period (they are usually the same)\n",
    "period_to_date = df_transcripts.groupby('reporting_period')['date_of_call'].min()\n",
    "\n",
    "# Step 2: Reorder the index of sentiment_summary by the corresponding call date\n",
    "sentiment_summary['date_of_call'] = sentiment_summary.index.map(period_to_date)\n",
    "sentiment_summary = sentiment_summary.sort_values('date_of_call')\n",
    "\n",
    "# Step 3: Drop the helper column (optional)\n",
    "sentiment_summary = sentiment_summary.drop(columns='date_of_call')\n",
    "\n",
    "# Step 4: Plot again\n",
    "sentiment_summary.plot(kind='bar', stacked=True, figsize=(12, 6),\n",
    "                       colormap='coolwarm', title='Sentiment by Quarter')\n",
    "plt.ylabel(\"Sentence Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e330dcc3",
   "metadata": {},
   "source": [
    "## By Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by reporting_period, bank, and sentiment\n",
    "sentiment_summary = df_transcripts.groupby(['reporting_period', 'bank', 'sentiment']).size()\n",
    "\n",
    "# Pivot to get sentiment categories as columns\n",
    "sentiment_summary = sentiment_summary.unstack(fill_value=0).reset_index()\n",
    "\n",
    "# Add date_of_call for chronological sorting\n",
    "date_map = df_transcripts.groupby('reporting_period')['date_of_call'].min()\n",
    "sentiment_summary['date_of_call'] = sentiment_summary['reporting_period'].map(date_map)\n",
    "\n",
    "# Sort by date\n",
    "sentiment_summary = sentiment_summary.sort_values('date_of_call')\n",
    "\n",
    "# Format labels\n",
    "sentiment_summary['pretty_label'] = sentiment_summary['reporting_period'].str.replace('_', ' ')\n",
    "\n",
    "# Plot per bank\n",
    "banks = sentiment_summary['bank'].unique()\n",
    "\n",
    "for bank in banks:\n",
    "    bank_data = sentiment_summary[sentiment_summary['bank'] == bank]\n",
    "    \n",
    "    # Set index for plotting\n",
    "    bank_data_plot = bank_data.set_index('pretty_label')\n",
    "    \n",
    "    # Drop helper columns\n",
    "    bank_data_plot = bank_data_plot.drop(columns=['reporting_period', 'bank', 'date_of_call'])\n",
    "\n",
    "    # Calculate percentages\n",
    "    percent_data = bank_data_plot.div(bank_data_plot.sum(axis=1), axis=0) * 100\n",
    "    colors = {\n",
    "        'Negative': '#1f77b4',  # Blue\n",
    "        'Neutral': '#708090',   # Orange\n",
    "        'Positive': '#2ca02c'   # Green\n",
    "    }\n",
    "\n",
    "    # Ensure columns match the colors\n",
    "    sentiment_order = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "    # Plot with custom colors\n",
    "    ax = bank_data_plot[sentiment_order].plot(\n",
    "        kind='bar', \n",
    "        stacked=True, \n",
    "        figsize=(12, 6),\n",
    "        color=[colors[s] for s in sentiment_order], \n",
    "        title=f'Sentiment by Quarter - {bank}'\n",
    "    )\n",
    "\n",
    "    plt.ylabel(\"Sentence Count\")\n",
    "    plt.xlabel(\"Period\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Annotate with percentages\n",
    "    for idx, row in enumerate(percent_data.values):\n",
    "        cumulative = 0\n",
    "        for j, value in enumerate(row):\n",
    "            if value > 5:  # Show only if > 5% to avoid clutter\n",
    "                y = cumulative + bank_data_plot.values[idx][j] / 2\n",
    "                ax.text(idx, y, f\"{value:.0f}%\", ha='center', va='center', fontsize=10, color='white')\n",
    "            cumulative += bank_data_plot.values[idx][j]\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71149adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_transcripts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb32c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transcripts.to_csv(\"data/app/data_nlp_analysis.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7245851",
   "metadata": {},
   "source": [
    "## ðŸ§  Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6adf7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_list_raw):\n",
    "    \"\"\"\n",
    "    Cleans a list of raw text by converting to lowercase and\n",
    "    and filtering out stop words.\n",
    "\n",
    "    Args:\n",
    "        text_list_raw: List of raw text strings.\n",
    "\n",
    "    Returns:\n",
    "        text_list_clean: List of cleaned text strings.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_list_clean = []\n",
    "\n",
    "    for text in text_list_raw:\n",
    "        if text and text.lower() != \"nan\":\n",
    "            text = text.lower()\n",
    "            word_tokens = word_tokenize(text)\n",
    "            filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "            cleaned_text = \" \".join(filtered_tokens)\n",
    "            text_list_clean.append(cleaned_text)\n",
    "\n",
    "    return text_list_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct some basic text cleaning\n",
    "clean_text = clean_text(df_transcripts['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8534925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use Fin-MPNET-Base for the embedding model in BERTopic as it hsa been tuned on financial data\n",
    "embedding_model_name = \"mukaj/fin-mpnet-base\"\n",
    "embedding_model = SentenceTransformer(embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e082aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTopic uses UMAP, a dimensionality reduction technique, to reduce the dimensionality of the embeddings before clustering\n",
    "# UMAP introduces stochastic behaviour so we'll set a random seed for reproducibility\n",
    "# https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html#preventing-stochastic-behavior\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=0) # These are the default parameters for UMAP used in _bertopic.py with the additional parameter of random_state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5398785",
   "metadata": {},
   "source": [
    "Let's take a look at the top topics ranked by the number of reviews they are assigned to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run BERTopic to extract topics from the cleaned text\n",
    "model = BERTopic(verbose=True, embedding_model=embedding_model, umap_model=umap_model)\n",
    "model.fit(clean_text)\n",
    "topics, probabilities = model.transform(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e1b20",
   "metadata": {},
   "source": [
    "## Response Investors Vs Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d2b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV (if not already loaded)\n",
    "df = pd.read_parquet(transcripts_FPATH)\n",
    "\n",
    "# Filter to Q&A section only\n",
    "df_qa = df[df['section'].str.contains(\"Q and A\", case=False, na=False)].copy()\n",
    "\n",
    "# Reset index for safe iteration\n",
    "df_qa = df_qa.reset_index(drop=True)\n",
    "\n",
    "# Define roles considered as management\n",
    "management_roles = ['CEO', 'CFO', 'Management', 'Executive']\n",
    "\n",
    "# Prepare output list\n",
    "qa_pairs = []\n",
    "\n",
    "# Slide through transcript row-by-row to detect Q -> A\n",
    "i = 0\n",
    "while i < len(df_qa) - 1:\n",
    "    row = df_qa.iloc[i]\n",
    "    next_row = df_qa.iloc[i + 1]\n",
    "\n",
    "    # Heuristic: if current is not host/management, and next is management â†’ Q&A\n",
    "    if row['role'] not in ['Host'] + management_roles and next_row['role'] in management_roles:\n",
    "        qa_pairs.append({\n",
    "            'question': row['text'],\n",
    "            'answer': next_row['text'],\n",
    "            'speaker_q': row['speaker'],\n",
    "            'speaker_a': next_row['speaker'],\n",
    "            'year': pd.to_datetime(row['date_of_call']).year,\n",
    "            'bank': row['bank'],\n",
    "            'reporting_period': row['reporting_period']\n",
    "        })\n",
    "        i += 2  # Skip to the next pair\n",
    "    else:\n",
    "        i += 1  # Slide forward if no match\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_qa_pairs = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# Preview\n",
    "print(df_qa_pairs.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f01865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Glossary Expansion for Acronyms ---\n",
    "glossary_df = pd.read_csv(GLOSSARY_FPATH)\n",
    "glossary_dict = glossary_df.set_index('Term')['Definition'].to_dict()\n",
    "\n",
    "def expand_acronyms(text, glossary):\n",
    "    for term, definition in glossary.items():\n",
    "        pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "        text = re.sub(pattern, definition, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "df_qa_pairs['question'] = df_qa_pairs['question'].apply(lambda q: expand_acronyms(str(q), glossary_dict))\n",
    "df_qa_pairs['answer'] = df_qa_pairs['answer'].apply(lambda a: expand_acronyms(str(a), glossary_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import re\n",
    "from umap import UMAP\n",
    "#from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "\n",
    "# Initialization\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "embedding_model = SentenceTransformer(\"mukaj/fin-mpnet-base\")\n",
    "\n",
    "# Load the processed DataFrame (replace with actual data path)\n",
    "# Expected columns: ['year', 'bank', 'transcript_id', 'question', 'answer', 'speaker_q', 'speaker_a']\n",
    "\n",
    "# --- Topic Modeling on Questions ---\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.7, top_n_words=10)\n",
    "#representation_model = KeyBERTInspired()\n",
    "\n",
    "#representation_model = KeyBERTInspired()\n",
    "questions = df_qa_pairs['question'].astype(str).tolist()\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=0)\n",
    "topic_model = BERTopic(embedding_model=embedding_model, umap_model=umap_model, nr_topics=25, representation_model=representation_model,\n",
    "verbose=True)\n",
    "topics, _ = topic_model.fit_transform(questions)\n",
    "df_qa_pairs['bertopic_topic'] = topics\n",
    "\n",
    "# --- Sentiment Analysis on Answers ---\n",
    "df_qa_pairs['sentiment'] = df_qa_pairs['answer'].apply(lambda x: sia.polarity_scores(str(x))['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_pairs['bertopic_label'] = df_qa_pairs['bertopic_topic'].apply(lambda x: topic_model.get_topic_info().set_index(\"Topic\").loc[x, \"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee35bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_qa_pairs['bertopic_label'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dab960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Expanded Topic Mapping with Semantic Similarity ---\n",
    "topic_mapping_dict = {\n",
    "    \"Capital Adequacy\": [\"capital\", \"tier 1 capital\", \"tier 2 capital\", \"risk-weighted assets\", \"capital buffer\", \"regulatory capital\", \"cet1 ratio\", \"capital ratio\", \"tangible book value\", \"leverage ratio\", \"basel iii\"],\n",
    "    \"Liquidity Risk\": [\"liquidity risk\", \"cash position\", \"short-term funding\", \"deposit outflows\", \"liquidity coverage ratio\"],\n",
    "    \"Profitability\": [\"profitability\", \"earnings\", \"revenue\", \"return on equity\", \"net income\", \"efficiency ratio\"],\n",
    "    \"Asset Quality and Credit Risk\": [\"non-performing\", \"loan loss\", \"credit risk\", \"charge-offs\", \"impairments\"],\n",
    "    \"Macroeconomic Risk\": [\"inflation\", \"recession\", \"monetary policy\", \"macroeconomic\"],\n",
    "    \"Interest Rate Risk\": [\"interest rate\", \"yield curve\", \"rate hikes\"],\n",
    "    \"Market and Volatility Risk\": [\"volatility\", \"market risk\", \"trading losses\"],\n",
    "    \"Operational Risk\": [\"cybersecurity\", \"system failure\", \"fraud\", \"technology risk\"],\n",
    "    \"Regulatory & Compliance Risk\": [\"compliance\", \"regulatory\", \"basel\", \"audit\", \"oversight\"],\n",
    "    \"ESG and Reputation Risk\": [\"esg\", \"sustainability\", \"climate\", \"governance\", \"reputation\"],\n",
    "    \"Strategic and Business Model Risk\": [\"strategy\", \"restructuring\", \"market entry\"],\n",
    "    \"Legal Risk\": [\"litigation\", \"lawsuit\", \"legal\", \"settlement\"]\n",
    "}\n",
    "\n",
    "# Prepare keyword embeddings\n",
    "expanded_topic_map = {\n",
    "    topic: embedding_model.encode([kw.lower() for kw in kws], convert_to_tensor=True)\n",
    "    for topic, kws in topic_mapping_dict.items()\n",
    "}\n",
    "\n",
    "def semantic_map_question(question, threshold=0.75):\n",
    "    question_embedding = embedding_model.encode(question.lower(), convert_to_tensor=True)\n",
    "    best_match, best_score = None, 0.0\n",
    "    for category, keyword_embeddings in expanded_topic_map.items():\n",
    "        scores = util.cos_sim(question_embedding, keyword_embeddings)\n",
    "        max_score = float(scores.max())\n",
    "        if max_score > best_score and max_score >= threshold:\n",
    "            best_match, best_score = category, max_score\n",
    "    return best_match if best_match else \"Unmapped\"\n",
    "\n",
    "df_qa_pairs['semantic_topic'] = df_qa_pairs['question'].apply(lambda q: semantic_map_question(q, threshold=0.2))\n",
    "df_qa_pairs['final_topic'] = df_qa_pairs.apply(lambda row: row['semantic_topic'] if row['semantic_topic'] != \"Unmapped\" else f\"BERTopic {row['bertopic_label']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_qa_pairs['semantic_topic'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_qa_pairs['final_topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d24aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# --- Helper functions ---\n",
    "def get_neutral_pastel_palette(n):\n",
    "    # Build from seaborn + matplotlib pastel-safe sets (manual override to avoid red/green)\n",
    "    safe_hex_colors = [\n",
    "        \"#aec6cf\", \"#cfcfc4\", \"#fdfd96\", \"#b39eb5\", \"#ffb347\",\n",
    "        \"#dda0dd\", \"#b0e0e6\", \"#cdb5cd\", \"#fab57a\", \"#d1cfe2\",\n",
    "        \"#e6e6fa\", \"#f5deb3\", \"#ccccff\", \"#e0bbE4\", \"#f7cac9\"\n",
    "    ]\n",
    "    return [mcolors.to_rgb(c) for c in safe_hex_colors[:n]]\n",
    "\n",
    "\n",
    "def sentiment_text_color(score):\n",
    "    if score >= 0.05:\n",
    "        return 'green'\n",
    "    elif score <= -0.05:\n",
    "        return 'red'\n",
    "    else:\n",
    "        return 'black'\n",
    "\n",
    "quarter_month_map = {'Q1': '03-31', 'Q2': '06-30', 'Q3': '09-30', 'Q4': '12-31'}\n",
    "def quarter_to_date(qr):\n",
    "    try:\n",
    "        q, y = qr.split('_')\n",
    "        return pd.to_datetime(f\"{y}-{quarter_month_map.get(q, '12-31')}\")\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "# --- Processing ---\n",
    "df_qa_pairs['date_of_call'] = df_qa_pairs['reporting_period'].apply(quarter_to_date)\n",
    "df_qa_pairs['quarter'] = df_qa_pairs['reporting_period']\n",
    "\n",
    "period_order = (\n",
    "    df_qa_pairs.groupby('quarter')['date_of_call']\n",
    "    .min()\n",
    "    .sort_values()\n",
    "    .index.tolist()\n",
    ")\n",
    "\n",
    "unique_topics = sorted(df_qa_pairs['final_topic'].unique())\n",
    "topic_colors = dict(zip(unique_topics, get_neutral_pastel_palette(len(unique_topics))))\n",
    "\n",
    "# --- Loop per bank ---\n",
    "for bank in df_qa_pairs['bank'].unique():\n",
    "    bank_df = df_qa_pairs[df_qa_pairs['bank'] == bank]\n",
    "\n",
    "    topic_summary = (\n",
    "        bank_df.groupby(['quarter', 'final_topic'])\n",
    "        .agg(count=('final_topic', 'size'), avg_sentiment=('sentiment', 'mean'))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    topic_summary['total'] = topic_summary.groupby('quarter')['count'].transform('sum')\n",
    "    topic_summary['proportion'] = topic_summary['count'] / topic_summary['total']\n",
    "    topic_summary['quarter'] = pd.Categorical(topic_summary['quarter'], categories=period_order, ordered=True)\n",
    "\n",
    "    pivot_df = topic_summary.pivot(index='quarter', columns='final_topic', values='proportion').fillna(0)\n",
    "    sentiment_lookup = topic_summary.set_index(['quarter', 'final_topic'])['avg_sentiment']\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    topic_patches = {}\n",
    "\n",
    "    for quarter in pivot_df.index:\n",
    "        bottom = 0\n",
    "        for topic in pivot_df.columns:\n",
    "            height = pivot_df.loc[quarter, topic]\n",
    "            if height > 0:\n",
    "                sentiment = sentiment_lookup.get((quarter, topic), 0)\n",
    "                face_color = topic_colors[topic]\n",
    "                label_color = sentiment_text_color(sentiment)\n",
    "\n",
    "                plt.bar(quarter, height, bottom=bottom, color=face_color)\n",
    "                if height > 0.03:\n",
    "                    plt.text(quarter, bottom + height / 2, f\"{sentiment:+.2f}\",\n",
    "                             ha='center', va='center', fontsize=8, color=label_color, fontweight='bold')\n",
    "\n",
    "                bottom += height\n",
    "                if topic not in topic_patches:\n",
    "                    topic_patches[topic] = mpatches.Patch(color=face_color, label=topic)\n",
    "\n",
    "    plt.title(f\"{bank} â€“ Topic Proportions per Quarter\\n(Topic's question of the analyst vs the bank sentimental response)\")\n",
    "    plt.ylabel(\"Proportion of Questions\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(handles=topic_patches.values(), title=\"Topics\", bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Risk Mapping ---\n",
    "risk_map = {\n",
    "    'Capital Adequacy': 'Capital Risk',\n",
    "    'Liquidity Risk': 'Liquidity Risk',\n",
    "    'Asset Quality and Credit Risk': 'Credit Risk',\n",
    "    'Interest Rate Risk': 'Market Risk',\n",
    "    'Market and Volatility Risk': 'Market Risk',\n",
    "    'Regulatory & Compliance Risk': 'Regulatory Risk',\n",
    "    'Operational Risk': 'Operational Risk',\n",
    "    'Macroeconomic Risk': 'Macro Risk',\n",
    "    'ESG and Reputation Risk': 'Reputation Risk',\n",
    "    'Strategic and Business Model Risk': 'Business Model Risk',\n",
    "    'Legal Risk': 'Legal Risk'\n",
    "}\n",
    "df_qa_pairs['supervisory_risk'] = df_qa_pairs['final_topic'].map(risk_map).fillna('Other')\n",
    "\n",
    "# --- Convert quarters to datetime ---\n",
    "quarter_month_map = {'Q1': '03-31', 'Q2': '06-30', 'Q3': '09-30', 'Q4': '12-31'}\n",
    "\n",
    "def quarter_to_date(qr):\n",
    "    try:\n",
    "        q, y = qr.split('_')\n",
    "        return pd.to_datetime(f\"{y}-{quarter_month_map.get(q, '12-31')}\")\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "df_qa_pairs['date_of_call'] = df_qa_pairs['reporting_period'].apply(quarter_to_date)\n",
    "df_qa_pairs['quarter'] = df_qa_pairs['reporting_period']\n",
    "\n",
    "# --- Sentiment aggregation ---\n",
    "risk_summary = (\n",
    "    df_qa_pairs.groupby(['supervisory_risk', 'quarter', 'date_of_call'])['sentiment']\n",
    "    .agg(['mean', 'count'])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# --- Create a categorical order for quarter based on date ---\n",
    "quarter_order = (\n",
    "    risk_summary.dropna(subset=['date_of_call'])\n",
    "    .drop_duplicates(subset=['quarter'])\n",
    "    .sort_values('date_of_call')['quarter']\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "risk_summary['quarter'] = pd.Categorical(risk_summary['quarter'], categories=quarter_order, ordered=True)\n",
    "\n",
    "# --- Plotting each risk separately with reporting_period on x-axis ---\n",
    "sns.set(style=\"whitegrid\")\n",
    "unique_risks = risk_summary['supervisory_risk'].unique()\n",
    "\n",
    "for risk in unique_risks:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    risk_data = risk_summary[risk_summary['supervisory_risk'] == risk].sort_values('quarter')\n",
    "    sns.lineplot(data=risk_data, x='quarter', y='mean', marker='o')\n",
    "    plt.title(f\"Average Sentiment Over Time â€“ {risk}\")\n",
    "    plt.ylabel(\"Mean Sentiment\")\n",
    "    plt.xlabel(\"Reporting Period\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e3e84",
   "metadata": {},
   "source": [
    "### Sentiment per Regulatory Topic per Quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Step 1: Create sentiment_label ---\n",
    "def classify_sentiment(score):\n",
    "    if score >= 0.2:\n",
    "        return \"Positive\"\n",
    "    elif score <= -0.2:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "df_qa_pairs['sentiment_label'] = df_qa_pairs['sentiment'].apply(classify_sentiment)\n",
    "\n",
    "# --- Step 2: Ensure correct time ordering ---\n",
    "quarter_month_map = {'Q1': '03-31', 'Q2': '06-30', 'Q3': '09-30', 'Q4': '12-31'}\n",
    "\n",
    "def quarter_to_date(qr):\n",
    "    try:\n",
    "        q, y = qr.split('_')\n",
    "        return pd.to_datetime(f\"{y}-{quarter_month_map.get(q, '12-31')}\")\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "df_qa_pairs['date_of_call'] = df_qa_pairs['reporting_period'].apply(quarter_to_date)\n",
    "df_qa_pairs['quarter'] = df_qa_pairs['reporting_period']\n",
    "df_qa_pairs['quarter'] = pd.Categorical(\n",
    "    df_qa_pairs['quarter'],\n",
    "    categories=sorted(df_qa_pairs['quarter'].dropna().unique(), key=quarter_to_date),\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# --- Step 3: Plot sentiment per bank ---\n",
    "for bank in df_qa_pairs['bank'].unique():\n",
    "    subset = df_qa_pairs[df_qa_pairs['bank'] == bank]\n",
    "    if subset.empty:\n",
    "        continue\n",
    "\n",
    "    sentiment_trend = (\n",
    "        subset.groupby('quarter')['sentiment_label']\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    sentiment_trend.plot(\n",
    "        kind='bar',\n",
    "        stacked=True,\n",
    "        figsize=(10, 5),\n",
    "        colormap='coolwarm'\n",
    "    )\n",
    "    plt.title(f\"Sentiment Distribution Over Time â€“ {bank}\")\n",
    "    plt.ylabel(\"Sentence Count\")\n",
    "    plt.xlabel(\"Reporting Period\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ffd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_qa_pairs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05dbff",
   "metadata": {},
   "source": [
    "### 4. Topic Drift (KL Divergence Between Quarters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e8fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define KL divergence\n",
    "def kl_divergence(p, q):\n",
    "    p = np.asarray(p) + 1e-10\n",
    "    q = np.asarray(q) + 1e-10\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "# Prepare KL divergence per bank\n",
    "kl_results = []\n",
    "\n",
    "for bank, bank_df in df_qa_pairs.groupby('bank'):\n",
    "    pivot = bank_df.groupby(['quarter', 'final_topic']).size().unstack(fill_value=0)\n",
    "    pivot_prop = pivot.div(pivot.sum(axis=1), axis=0)\n",
    "    \n",
    "    # Skip banks with < 2 quarters\n",
    "    if len(pivot_prop) < 2:\n",
    "        continue\n",
    "\n",
    "    for i in range(1, len(pivot_prop)):\n",
    "        prev_dist = pivot_prop.iloc[i - 1]\n",
    "        curr_dist = pivot_prop.iloc[i]\n",
    "        kl_val = kl_divergence(prev_dist, curr_dist)\n",
    "        kl_results.append({\n",
    "            'bank': bank,\n",
    "            'quarter': pivot_prop.index[i],\n",
    "            'KL Divergence': kl_val\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "kl_df = pd.DataFrame(kl_results)\n",
    "\n",
    "# --- Sort quarter categorically for proper order ---\n",
    "quarter_month_map = {'Q1': '03-31', 'Q2': '06-30', 'Q3': '09-30', 'Q4': '12-31'}\n",
    "def quarter_to_date(qr):\n",
    "    try:\n",
    "        q, y = qr.split('_')\n",
    "        return pd.to_datetime(f\"{y}-{quarter_month_map.get(q, '12-31')}\")\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "kl_df['quarter_date'] = kl_df['quarter'].apply(quarter_to_date)\n",
    "ordered_quarters = (\n",
    "    kl_df[['quarter', 'quarter_date']]\n",
    "    .drop_duplicates()\n",
    "    .sort_values('quarter_date')['quarter']\n",
    "    .tolist()\n",
    ")\n",
    "kl_df['quarter'] = pd.Categorical(kl_df['quarter'], categories=ordered_quarters, ordered=True)\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=kl_df,\n",
    "    x='quarter',\n",
    "    y='KL Divergence',\n",
    "    hue='bank',\n",
    "    palette='tab10'  # Use 'tab10', 'Set2', or 'Dark2' for clear distinctions\n",
    ")\n",
    "\n",
    "plt.title(\"Topic Distribution Drift Over Time by Bank\", fontsize=14)\n",
    "plt.xlabel(\"Quarter\", fontsize=12)\n",
    "plt.ylabel(\"KL Divergence\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Bank', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b63605",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_pairs = df_qa_pairs[~df_qa_pairs['final_topic'].str.startswith(\"BERTopic\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c541352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Peer Comparison ---\n",
    "peer_avg = df_qa_pairs.groupby(['bank', 'final_topic'])['sentiment'].mean().unstack()\n",
    "sns.heatmap(peer_avg, annot=True, center=0, cmap='RdYlGn', fmt=\".2f\").set_title(\"Topic Sentiment by Bank\")\n",
    "plt.ylabel(\"Banks\")\n",
    "plt.xlabel(\"Topics\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960cdaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_qa_pairs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_pairs.to_csv(\"data/app/data_qa_pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a193a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_pairs.to_parquet(\"data/app/data_qa_pairs.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68cd99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
